{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import *\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "folder_path = 'articles_all'\n",
    "file_name = \"collated_data.txt\"\n",
    "inverted_index_file = \"inverted_index.txt\"\n",
    "file_contents_list = []\n",
    "\n",
    "\n",
    "file_name_number_mapping_dict = {}\n",
    "number_file_name_mapping_dict = {}\n",
    "inverted_index_intermediate = {}\n",
    "inverted_index = {}\n",
    "num_docs = 1\n",
    "idf_dict = {}\n",
    "tf_dict = {}\n",
    "file_to_vec_dict = {}\n",
    "query_to_vec_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fileContents:\n",
    "    file_name = \"\"\n",
    "    file_number = 0\n",
    "    contents_raw = \"\"\n",
    "    contents_tokens = []\n",
    "\n",
    "class sentenceMatch:\n",
    "    file_name = 0\n",
    "    match = 0\n",
    "    sentence_raw = \"\"\n",
    "    sentence_tokenised = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFileOnly(file_num):\n",
    "\n",
    "    for entry in file_contents_list:\n",
    "        if entry.file_number == int(file_num):\n",
    "            file_name = entry.file_name\n",
    "\n",
    "    # file_name = number_file_name_mapping_dict[str(file_num)]\n",
    "    file_path = folder_path + '/' + file_name\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    data = re.sub(\"\\\\n\", \"\", data)\n",
    "    data = re.sub(\"'''\", \"\", data)\n",
    "    data = re.sub(\"''\", \"\", data)\n",
    "    data = re.sub(\",\", \"\", data)\n",
    "\n",
    "    data_list = data.split('.')\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "def tokenize_and_remove_punctuations(s):\n",
    "    translator = str.maketrans('','',string.punctuation)\n",
    "    modified_string = s.translate(translator)\n",
    "    modified_string = ''.join([i for i in modified_string if not i.isdigit()])\n",
    "    return nltk.word_tokenize(modified_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    stop_words = [word for word in open('stopwords.txt','r').read().split('\\n')]\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(contents):\n",
    "    contents = contents.lower()\n",
    "    title_start = contents.find('<title>')\n",
    "    title_end = contents.find('</title>')\n",
    "    title = contents[title_start+len('<title>'):title_end]\n",
    "    text_start = contents.find('<text>')\n",
    "    text_end = contents.find('</text>')\n",
    "    text = contents[text_start+len('<text>'):text_end]\n",
    "    return title+\" \"+text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):\n",
    "    stop_words = get_stopwords()\n",
    "    filtered_words = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(data):\n",
    "    tokens = []\n",
    "    for token_list in data.values():\n",
    "        tokens = tokens + token_list\n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    return list(fdist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(contents):\n",
    "    dataDict = {}\n",
    "    data_list = []\n",
    "    for content in contents:\n",
    "\n",
    "        tokens = tokenize_and_remove_punctuations(content)\n",
    "        filtered_tokens = remove_stop_words(tokens)\n",
    "        stemmed_tokens = stem_words(filtered_tokens)\n",
    "\n",
    "        if len(stemmed_tokens) !=  0:\n",
    "            data_list.append(stemmed_tokens)\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    curr_file_num = 1\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "        # print (filename)\n",
    "        if curr_file_num%1000 == 0:\n",
    "            print (curr_file_num)\n",
    "\n",
    "        global file_contents_list\n",
    "\n",
    "        file_data = fileContents()\n",
    "        # contents = []\n",
    "        \n",
    "        data = parse_data(open(path + '/' + filename,'r').read())\n",
    "    \n",
    "        data = data.replace('\\n', ' ').replace(\"'''\", '').replace(\"''\", '').replace(\",\", ' ').strip()\n",
    "\n",
    "        # print (data)\n",
    "        file_data.contents_raw = data\n",
    "        file_data.file_num = curr_file_num\n",
    "        file_data.file_name = filename\n",
    "\n",
    "        # filename = re.sub('\\D',\"\",filename)\n",
    "        # contents.append([file_name_number_mapping_dict[filename],data])\n",
    "        # print (data)\n",
    "    # return contents\n",
    "\n",
    "        # print (type(data), data)\n",
    "        data_list = data.split()\n",
    "        file_data.contents_tokens = preprocess_data(data_list)\n",
    "\n",
    "        file_contents_list.append(file_data)\n",
    "\n",
    "        # inverted_index = generate_inverted_index(preprocess_data_dict)\n",
    "        curr_file_num += 1\n",
    "    return  inverted_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_vec(queries):\n",
    "    global query_to_vec_dict\n",
    "\n",
    "    for key, val in queries.items():\n",
    "        # query_to_vec_dict[key] = []\n",
    "        val_set = set(val)\n",
    "        for tok in val_set:\n",
    "            count = val.count(tok)\n",
    "            query_to_vec_dict[tok] = count\n",
    "\n",
    "    # print(key, val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_queries(path):\n",
    "    queriesDict = {}\n",
    "    queries = open(path,'r').read().split('\\n')\n",
    "    i = 1\n",
    "    for query in queries:\n",
    "        tokens = tokenize_and_remove_punctuations(query)\n",
    "        filtered_tokens = remove_stop_words(tokens)\n",
    "        stemmed_tokens = stem_words(filtered_tokens)\n",
    "        filtered_tokens1 = remove_stop_words(stemmed_tokens)\n",
    "        queriesDict[i] = filtered_tokens1\n",
    "        i+=1\n",
    "    return queriesDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSentences(contents):\n",
    "    # dataDict = {}\n",
    "    final_tokens = []\n",
    "    \n",
    "    for content in contents:\n",
    "        tokens = tokenize_and_remove_punctuations(content)\n",
    "        filtered_tokens = remove_stop_words(tokens)\n",
    "        stemmed_tokens = stem_words(filtered_tokens)\n",
    "        filtered_tokens1 = remove_stop_words(stemmed_tokens)\n",
    "        # dataDict[content[0]] = filtered_tokens1\n",
    "        \n",
    "        \n",
    "        for tok in filtered_tokens1:\n",
    "            # print (len(tok), tok)\n",
    "            if len(tok) != 0:\n",
    "                final_tokens.append(tok)\n",
    "\n",
    "    # print (final_tokens)\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read tokensied data from file \n",
    "with open(file_name, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    content = line.split(\"!@#\")\n",
    "    file_content = fileContents()\n",
    "    file_content.file_name = content[0]\n",
    "    file_content.file_number = int(content[1])\n",
    "    file_content.contents_raw = content[2]\n",
    "    file_content.contents_tokens = content[3].replace(\"'], ['\", \",\").replace(\"\\n\", \"\").strip(\"'[]'\").split(',')\n",
    "\n",
    "    file_contents_list.append(file_content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inverted index\n",
    "\n",
    "with open(inverted_index_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    content = line.split(\"\\t\")\n",
    "    inverted_index_list = content[1].strip('\\n').strip(\"[]\").replace(\"], [\", \":\").split(':')\n",
    "    inverted_index_list  = [x.split(',') for x in inverted_index_list]\n",
    "    \n",
    "    for pair in inverted_index_list:\n",
    "        pair[0] = int(pair[0])\n",
    "        pair[1] = int(pair[1].strip())\n",
    "\n",
    "    inverted_index[content[0]] = inverted_index_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadQuery():\n",
    "    global query_to_vec_dict\n",
    "    query_to_vec_dict = {}\n",
    "\n",
    "    queries_dict = preprocess_queries('queries.txt')\n",
    "    # print (queries_dict)\n",
    "    query_to_vec(queries_dict)\n",
    "\n",
    "    print (\"Tokenised query along with frequency\")\n",
    "    for key, val in query_to_vec_dict.items():\n",
    "        print (key, val)\n",
    "\n",
    "    return query_to_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndexForQuery(query_to_vec_dict):\n",
    "\n",
    "    index_for_query_dict = {}\n",
    "    index_for_query_list = []\n",
    "\n",
    "    for tok in query_to_vec_dict:\n",
    "\n",
    "        if tok in inverted_index.keys():\n",
    "            doc_list = inverted_index[tok]\n",
    "            len_doc_list = len(doc_list)\n",
    "\n",
    "            for pair in doc_list:\n",
    "                pair[1] = pair[1]/(len_doc_list)\n",
    "                \n",
    "                # print (type(pair[0]))  \n",
    "\n",
    "            for doc in doc_list:\n",
    "                if doc[0] in index_for_query_dict.keys():\n",
    "                    index_for_query_dict[doc[0]] += doc[1]\n",
    "                else:\n",
    "                    index_for_query_dict[doc[0]] = doc[1]\n",
    "\n",
    "    \n",
    "    for key, val in index_for_query_dict.items():\n",
    "        index_for_query_list.append([key, val])\n",
    "    \n",
    "    index_for_query_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    index_for_query_list = index_for_query_list[:25]\n",
    "\n",
    "    print()\n",
    "    print (\"Index retrieved for query\")\n",
    "    print ((index_for_query_list))\n",
    "\n",
    "    return index_for_query_list\n",
    "# print (len(index_for_query[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSentences(index_for_query_list):\n",
    "    sentences_list = []\n",
    "\n",
    "    global file_contents_list\n",
    "\n",
    "    for entry in index_for_query_list: \n",
    "        sentences = readFileOnly(entry[0])\n",
    "        \n",
    "        for line in sentences:\n",
    "            sentence_match = sentenceMatch()\n",
    "            sentence_match.sentence_raw = line.strip()\n",
    "\n",
    "            for content in file_contents_list:\n",
    "                if content.file_number == entry[0]:\n",
    "                    sentence_match.file_name = content.file_name\n",
    "\n",
    "            sentence_match.sentence_tokenised = processSentences(sentence_match.sentence_raw.split())\n",
    "            sentences_list.append(sentence_match)\n",
    "    \n",
    "    # for entry in sentences_list:\n",
    "    #     print (entry.fil\n",
    "    for entry in index_for_query_list: \n",
    "        sentences = readFileOnly(entry[0])\n",
    "        \n",
    "        for line in sentences:\n",
    "            sentence_match = sentenceMatch()\n",
    "            sentence_match.sentence_raw = line.strip()\n",
    "\n",
    "            for content in file_contents_list:\n",
    "                if content.file_number == entry[0]:\n",
    "                    sentence_match.file_name = content.file_name\n",
    "\n",
    "            # print (sentence_match.file_name, sentence_match.sentence_raw)\n",
    "            sentence_match.sentence_tokenised = processSentences(sentence_match.sentence_raw.split())\n",
    "            # print (type(sentence_match.sentence_tokenised), sentence_match.sentence_tokenised)\n",
    "            # processSentences(sentence_match.sentence_raw.split())\n",
    "            sentences_list.append(sentence_match)\n",
    "    \n",
    "\n",
    "    return sentences_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankSentences(query_to_vec_dict, sentences_list):\n",
    "    query_token = []\n",
    "\n",
    "    for key in query_to_vec_dict:\n",
    "        query_token.append(key)\n",
    "\n",
    "    # print (query_token)\n",
    "\n",
    "    for entry in sentences_list:\n",
    "        match_count = 0\n",
    "        for tok in query_token:\n",
    "            if tok in entry.sentence_tokenised:\n",
    "                match_count += 1*query_to_vec_dict[tok]\n",
    "        entry.match = match_count\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTopSentences(sentences_list):\n",
    "    res = sorted(sentences_list, key = lambda sentenceMatch : sentenceMatch.match, reverse=True)\n",
    "\n",
    "    res = res[:10]\n",
    "\n",
    "    print (\"The obtained sentences are:\")\n",
    "    for entry in res:\n",
    "        print (entry.file_name, entry.sentence_raw)\n",
    "        print ()\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenised query along with frequency\n",
      "ship 1\n",
      "titan 1\n",
      "sank 1\n",
      "alien 1\n",
      "collid 1\n",
      "\n",
      "Index retrieved for query\n",
      "[[34519, 0.46987881796151443], [53983, 0.41981789568345323], [39377, 0.2892165679221329], [21716, 0.23046875], [34620, 0.2136267456622937], [58419, 0.1796875], [7764, 0.1704109587249399], [11929, 0.162251375370292], [5251, 0.13651079136690647], [28765, 0.13598901098901098], [45803, 0.13165665996614473], [51678, 0.13090383588325846], [20318, 0.10480349344978165], [36465, 0.10300134892086331], [16739, 0.09894756683735981], [9869, 0.09606986899563319], [44620, 0.08733624454148471], [12116, 0.0845728711790393], [60903, 0.08100269784172662], [60681, 0.07860262008733625], [67220, 0.07778247989843419], [52225, 0.07202708421498095], [67939, 0.07175134892086331], [40743, 0.07151925518408803], [9676, 0.06593406593406594]]\n",
      "The obtained sentences are:\n",
      "Ship.txt At one time the steamships RMS Titanic|Titanic Olympic and Britannic were the largest ships in the world Titanic sank on her maiden voyage after hitting an iceberg becoming one of the most famous shipwrecks of all time the Olympic was Titanic 's nearly identical twin and actually set sail before Titanic and was scrapped in the 1930s after a very successful career including her being a passenger liner and a warship in World War I\n",
      "\n",
      "Ship.txt At one time the steamships RMS Titanic|Titanic Olympic and Britannic were the largest ships in the world Titanic sank on her maiden voyage after hitting an iceberg becoming one of the most famous shipwrecks of all time the Olympic was Titanic 's nearly identical twin and actually set sail before Titanic and was scrapped in the 1930s after a very successful career including her being a passenger liner and a warship in World War I\n",
      "\n",
      "Skillet.txt A Little More (originally was supposed to be on Alien Youth but didn't make the record so he re-recorded the song and released it on Collide but there is a little clip of the original recording as a hidden track)= Collide and the Savior music video (2004-2005) =On May 25 2004 Collide was released by Ardent Records and Lava Records\n",
      "\n",
      "Skillet.txt A Little More (originally recorded for Alien Youth but didn't get released on the record so he re-recorded it and released it on Collide)9\n",
      "\n",
      "Skillet.txt Kasica was with the band for 9 years contributing to the albums Alien Youth Collide Comatose and Awake as well as the live album Comatose Comes Alive\n",
      "\n",
      "Skillet.txt I Want To LiveDiscography= Studio albums = 1996: Skillet (album)|Skillet (Grunge) 1998: Hey You I Love Your Soul (Industrial) 2000: Invincible (Skillet album)|Invincible (Industrial) 2002: Alien Youth (Industrial) 2004: Collide (Skillet album)|Collide (Nu metal) 2006: Comatose (Skillet album)|Comatose (Symphonic rock) 2009: Awake (Skillet album)|Awake (Symphonic rock) 2013: Rise (Skillet album)|Rise (Symphonic rock) 2016: Unleashed (Skillet album)|Unleashed (Symphonic rock)= Worship albums = 2000: Ardent Worship Singles = Late 1990s =1997: I Can Gasoline My Beautiful Robe Saturn (Skillet)1998: Hey You I Love Your Soul More Faithful Locked in a Cage Suspended in You (Hey You I Love Your Soul)1999: Take Whirlwind (Hey You I Love Your Soul)= Early 2000s =2000: Invincible Best Kept Secret You're Powerful Rest The One Each Other The Fire Breathes (Invincible)2001: You're In My Brain Angels Fall Down Always The Same (Invincible)2002: Alien Youth Stronger Rippin' Me Off You Are My Hope (Alien Youth)2003: Vapor Kill Me Heal Me Eating Me Away One Real Thing (Alien Youth)= Mid 2000s =2004: Forsaken My Obsession Savior Open Wounds (Collide)2005: Collide Under My Skin A Little More (Collide)2006: Comatose Rebirthing (Comatose)= Late 2000s =2007: Whispers In The Dark The Older I Get (Comatose)2008: Better Than Drugs Yours To Hold Falling Inside The Black Those Nights (Comatose)2009: The Last Night (Comatose) Hero Monster (Awake)= Early 2010s =2010: Awake and Alive Dead Inside (Awake)2011: It's Not Me It's You Lucy Forgiven One Day Too Late (Awake)2012: Should've When You Could've (Awake)2013: Sick Of It Rise Not Gonna Die Fire and Fury (Rise)= Mid 2010s =2014: What I Believe Madness In Me American Noise Circus For A Physco (Rise)2015: Salvation (Rise)2016: Feel Invincible Back From The Dead Stars The Resistance (Unleashed)= Late 2010s =2017: Breaking Free Out Of Hell Lions Burn It Down (Unleashed)2018: I Want To Live (Unleashed) Tours = Skillet Tour (1996-97) = Setlist  Gasoline Saturn My Beautiful Robe Safe With You I CanThe Skillet Tour was a Skillet tour promoting their album Skillet It went from 1996 to 1997\n",
      "\n",
      "Skillet.txt = Collide Tour (2004-05) = June 2004 to February 2005 Setlist  My Obsession Energy Best Kept Secret Collide Forsaken Always The Same A Little More You're Powerful Alien Youth Fingernails Locked in a Cage Kill Me Heal Me Heaven In My Veins Invincible Angels Fall Down Open Wounds Saturn More Faithful Imperfection Under My Skin Will You Be There Savior I Can July to September 2005 Setlist  Energy My Obsession Best Kept Secret Always The Same Kill Me Heal Me Saturn Alien Youth Locked in a Cage More Faithful Collide Forsaken A Little More Heaven In My Veins Be Thou My Vision Angels Fall Down Savior I CanThe Collide Tour was a Skillet tour promoting their album Collide It went from June 6 2004 to September 23 2005\n",
      "\n",
      "List of Percy Jackson and the Olympians characters.txt Percy asks him to sink all of the Titan ships that come in his river and splits the sand dollar his father gave him for his fifteenth birthday with East River and Hudson so they both agree to his request\n",
      "\n",
      "List of Percy Jackson and the Olympians characters.txt Percy asks him to sink all of the Titan    ships that come in his river and splits the sand dollar his father gave him for his fifteenth birthday with East River and Hudson so they both agree to his request\n",
      "\n",
      "Titanic.txt Before the Titanic sailed many people thought it would be almost impossible for ships of this design to sink\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_dict = loadQuery()\n",
    "index_for_query_list = getIndexForQuery(query_dict)\n",
    "sentences = extractSentences(index_for_query_list)\n",
    "rankSentences(query_dict, sentences)\n",
    "printTopSentences(sentences)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
